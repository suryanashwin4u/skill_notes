        lectures videos notes:
-------------------------------------
anaconda navigators:
-------------------------------------
1.spyder ide 
2.jupyter notebook ide
-------------------------------------
important python libraries:
-------------------------------------
1.numpy
2.pandas
3.matplotlib
4.seaborn
5.scikit-learn
-------------------------------------
supervised learning are of 2 Types
-------------------------------------
supervised learning is when the model is getting trained on a labelled dataset.

1.regression:

1.linear regression(
    model representation,
    cost function
    )

2.optimization(
    gradient descent,
    batch & mini batch,
    stochastic,
    confusion matrix,
    bias variance trade off,
    multiple linear regression,
    polynomial regression,
    overfitting and underfitting, 
    support vector regression,
    decision tree regression,
    random forest regression
    )

2.classification:
Logistic Regression,
k-nearest neighbors,
support vector machine:kernel svm,
naive bayes,
decision tree classification,
random forest classification

-------------------------------------
unsupervised learning:
-------------------------------------

1.clustering:
k-means clustering,
hierarchical clustering

2.association rule mining:
apriori,
eclat

3.dimensionality reduction:
principal component analysis
linear discriminant analysis
kernel pca 

4.regularization:
ridge,
lasso

5.model selection:
kfold cross validation,
grid search

6.ensemble learning:
bagging & boosting:xgboost

-------------------------------------
3.reinforcement learning:
-------------------------------------
the process of encouraging or establishing a belief or pattern of behaviour.

an agent learns to behave in a environment by performing actions and seeing the results.
  
upper confidence bound(ucb),
thomson sampling

------------------------------------- 
installation guide anaconda navigator
-------------------------------------
1.go to anaconda website and download anaconda. 
2.open it and search for jupyter notebook.
3.click launch button and it will start localhost:8888 on browser automatically 

or

if you have already installed jupyter notebook then just run jupyter notebook command from cmd

-------------------------------------
How does machine learning works:
-------------------------------------
machine learning algos is trained using a training data set to create a models.
when new input data(test data) is introduced to the ML algo, it makes a prediction on the basis of the models.

The prediction is evaluated for accuracy and if the accuracy is acceptable, the machine learning algo is deployed.
if the accuracy is not acceptable, the machine learning algorithm is trained again and again with an augmentaed training data set.
-------------------------------------
data mining:the process of discovering patterns in large data sets involving methods at the intersection of machine learning,
statistics and database systems.

data mining is the analysis step of the knowledge discovery in databases or kdd

predictive modelling: a process that uses data mining and probability to forecast outcomes
-------------------------------------
artificial intelligence > machine learning(subset) > deep learning(subset)

AI coined by John McCarthy 

2 types of AI: 
1.Applied AI(Weak AI)
2.Generalized AI(Strong AI)

for example: 
1.weak AI (ALEXA,GOOGLE ASSISTANT)
2.strong AI(human)

machine is a subset of AI
machine learning is a set of algorithms that train on a data set to make predictions or take actions in order to optimize some systems.

deep learning is a subset of machine learning where learning method is based on data representation or feature learning.

deep refers to 1 or more hidden layers in this case.

in deep learning dat goes through many numbers of non-linear transformation obtain an output.

examples: google lens, fb face recognition, mobile check deposits

differences between Ml vs Dl:
1.data dependencies:dl needs large data
2.hardware dependencies:dl needs high power devices
3.feature engineering:dl needs features as information
4.execution time: dl needs more time

AI:human intelligence exhibited by machines
ML:an approach to achieve artificial intelligence
DL:a technique for implementing machine learning

data science has an intersection with artificial intelligence but is not a subset of artificial intelligence.

data science is a subset of bit of AI,ML,DL

data science is the art and science of drawing actionable insights from the data.

applications: retail,bank,ecommerce,healthcare and telecom etc.

steps in datascience: 
1.data collection
2.data preprocessing
3.data analyzing
4.data visualization
5.model building:[importing data, data cleaning, model building, train model, test model, improve efficiancy, prediction]
6.deployment
 
-------------------------------------

machine learning life cycle:
1.define project objectives
2.data collection:[primary data:collected by researcher first hand,secondary data:passed through statistical process]
3.data preprocessing:[data cleaning(filling missing data,smoothing noisy data),transformation(normalisation),dimensionality reduction]
4.data visualization
5.model selection
6.model building
7.deployment

-------------------------------------

numpy is scientific computing library for python
support large number of data in the form of multi-dimensional array and matrix
use for maths calculation like linear algebra, Fourier transform , and random number capabilites

-------------------------------------

tab separated -> .tsv
comma separated -> .csv

-------------------------------------

install numpy package : pip install numpy 
to import into project : import numpy as np
create numpy array : np.array([1,2,3,4,5])
create numpy 2d array : np.array([1,2,3,4,5],[1,2,3])
create numpy 3d array : np.array([1,2,3,4,5],[1,2,3],[2,3,4])
create numpy Nd array : np.array([1,2,3,4,5],[1,2,3],[2,3,4].......[],[],[])
check type : type(variable)
check dimension : array_var.ndim
check size : array_var.size
check shape : array_var.shape
check datatype : array_var.dtype
create ones 1d array : array_var.ones(5)
create ones 2d array : array_var.ones((3,4))
create ones 3d array : array_var.ones((3,4,3))









-------------------------------------

execute current cell and create a new one down below
=>shift + enter
execute only current cell
=>control + enter
press A to create a new cell

-------------------------------------
numpy codes:













1.Pandas is used for reading data and data manipulation.

2.numpy is used for computations of numerical data.

3.matplotlib is used for graphing data.

4.scikit-learn is used for machine learning models. 

5.Supervised learning is when we have a known target based on past data (for example, predicting what price a house will sell for).

6.unsupervised learning is when there isn't a known past answer (for example, determining the topics discussed in restaurant reviews).

7.Within supervised learning, there are classification and regression problems. 

8.Regression is predicting a numerical value (for example, predicting what price a house will sell for).

9.classification is predicting what class something belongs to (for example, predicting if a borrower will default on their loan).

10.machine learning models:  Logistic Regression, Decision Trees, Random Forests, Neural Networks.

11.The median can also be thought of as the 50th percentile. This means that 50% of the data is less than the median and 50% of the data is greater than the median. This tells us where the middle of the data is, but we often want more of an understanding of the distribution of the data. We�ll often look at the 25th percentile and the 75th percentile.The 25th percentile is the value that�s one quarter of the way through the data. This is the value where 25% of the data is less than it (and 75% of the data is greater than it).Similarly, the 75th percentile is three quarters of the way through the data. This is the value where 75% of the data is less than it (and 25% of the data is greater than it).

12.If there is an even number of datapoints, to find the median (or 50th percentile), you take the mean of the two values in the middle.

13.The standard deviation and variance are measures of how dispersed or spread out the data is, We measure how far each datapoint is from the mean.

14.Even though data is never a perfect normal distribution, we can still use the standard deviation to gain insight about how the data is distributed.

15.standard deviation is square root of variance.

16.use numpy for manipulating arrays.

17.First we import the package. It is standard practice to nickname numpy as np.

for example:-

import numpy as np
data = [15, 16, 18, 19, 22, 24, 29, 30, 34]
print("mean:", np.mean(data))
print("median:", np.median(data))
print("50th percentile (median):", np.percentile(data, 50))
print("25th percentile:", np.percentile(data, 25))
print("75th percentile:", np.percentile(data, 75))
print("standard deviation:", np.std(data))
print("variance:", np.var(data))

18.Pandas is a Python module that helps us read and manipulate data. What's cool about pandas is that you can take in data and view it as a table that's human readable, but it can also be interpreted numerically so that you can do lots of computations with it.

19.We call the table of data a DataFrame.

20.We need to start by importing Pandas. It's standard practice to nickname it pd so that it's faster to type later on.

21.Our data is stored as CSV (comma-separated values) file.

22.The first line is the header and then each subsequent line is the data in csv file.

23.read data from csv file and display using pandas function:
for example:-

import pandas as pd
df=pd.read_csv("give path to csv file")             //read csv file and store into object
print(df)                                           //print all rows at once
print(df.head())                                    //print only 5 rows
print(df['column_name'])                            //print single column values
print(df[['column_name_1','column_name_2',....]])   //print multiple columns values

24.describe method in python will show results like:

Count: This is the number of rows that have a value. 
Mean: Recall that the mean is the standard average.
Std: This is short for standard deviation. This is a measure of how dispersed the data is.
Min: The smallest value
25%: The 25th percentile
50%: The 50th percentile, also known as the median.
75%: The 75th percentile
Max: The largest value

for example:-
import pandas as pd
pd.options.display.max_columns = 6                                            //define max no. of columns to show
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
print(df.describe())                                                          //describe every result like mean,median,std,var,percentiles

25.We create a Pandas Series that will be a series of Trues and Falses:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
print(df['Sex'] == 'male')                                                   //return true or false series
df['male'] = df['Sex'] == 'male'                                             //return a column carries true or false and make a new column

26.Numpy is a Python package for manipulating lists and tables of numerical data. We can use it to do a lot of statistical calculations. We call the list or table of data a numpy array. so, numpy object is an array

27.We often start with our data in a Pandas DataFrame, but then want to convert it to a numpy array. The values attribute does this for us.

for example:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
print(df['Fare'].values)

for example:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
print(df[['Pclass', 'Fare', 'Age']].values)

28.We use the numpy shape attribute to determine the size of our numpy array. The size tells us how many rows and columns are in our data.

for example:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
arr = df[['Pclass', 'Fare', 'Age']].values
print(arr.shape)

29.